import csv
import os
import sys

# --- CONFIGURATION ---
INPUT_CSV = "residue_detailed_analysis_inchikey.csv"  # The file generated by the previous script
OUTPUT_CSV = "residue_detailed_analysis_inchikey_unique.csv"            # The clean output file
HASH_COLUMN = "MOL2_SHA256_Hash"                      # The exact header name of the hash column

def deduplicate_csv():
    if not os.path.exists(INPUT_CSV):
        print(f"Error: Input file '{INPUT_CSV}' not found.")
        return

    print(f"Reading from: {INPUT_CSV}")
    print(f"Writing to:   {OUTPUT_CSV}")
    print("-" * 40)

    seen_hashes = set()
    total_rows = 0
    unique_rows = 0
    duplicates_removed = 0

    try:
        with open(INPUT_CSV, mode='r', newline='', encoding='utf-8') as infile, \
             open(OUTPUT_CSV, mode='w', newline='', encoding='utf-8') as outfile:
            
            # Use DictReader/DictWriter to handle columns by name
            reader = csv.DictReader(infile)
            
            # verify hash column exists
            if HASH_COLUMN not in reader.fieldnames:
                print(f"Error: Column '{HASH_COLUMN}' not found in CSV.")
                print(f"Available columns: {reader.fieldnames}")
                return

            writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)
            writer.writeheader()

            for row in reader:
                total_rows += 1
                current_hash = row[HASH_COLUMN]

                # --- THE DEDUPLICATION LOGIC ---
                # "Hash_Error" implies a read failure, usually we keep these to check later,
                # or you can treat them as unique to avoid deleting data.
                # Here, we treat valid duplicates based on the hash string.
                if current_hash not in seen_hashes:
                    writer.writerow(row)
                    seen_hashes.add(current_hash)
                    unique_rows += 1
                else:
                    duplicates_removed += 1

    except Exception as e:
        print(f"An error occurred: {e}")
        return

    print("-" * 40)
    print(f"Total rows processed:     {total_rows}")
    print(f"Unique rows retained:     {unique_rows}")
    print(f"Duplicate rows removed:   {duplicates_removed}")
    print("-" * 40)
    print(f"Cleanup Complete.")

if __name__ == "__main__":
    deduplicate_csv()